{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data/')\n",
    "LABEL_PATH = Path('./labels/')\n",
    "\n",
    "AUG_DATA_PATH = Path('./data/data_augmentation/')\n",
    "\n",
    "MODEL_PATH=Path('./models/')\n",
    "LOG_PATH=Path('./logs/')\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "model_state_dict = None\n",
    "BERT_PRETRAINED_PATH = Path('./models/base_model')\n",
    "LOG_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = MODEL_PATH/'output'\n",
    "\n",
    "FINETUNED_PATH = OUTPUT_PATH/'model_out' \n",
    "\n",
    "\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Box({\n",
    "    \"run_text\": \"multilabel toxic comments with freezable layers\",\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"log_path\": LOG_PATH,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"task_name\": \"toxic_classification_lib\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": OUTPUT_PATH,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 6,\n",
    "    \"warmup_proportion\": 0.0,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_steps\": -1,\n",
    "    \"multi_gpu\": False,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \"eval_all_checkpoints\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"seed\": 42,\n",
    "    \"loss_scale\": 128,\n",
    "    \"task_name\": 'intent',\n",
    "    \"model_name\": 'bert-base-cased',\n",
    "    \"model_type\": 'bert'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(logfile),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str(FINETUNED_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/06/2020 03:02:23 - INFO - transformers.configuration_utils -   loading configuration file models/output/model_out/config.json\n",
      "05/06/2020 03:02:23 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 6,\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   Model name 'models/output/model_out' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'models/output/model_out' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   Didn't find file models/output/model_out/added_tokens.json. We won't load it.\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   loading file models/output/model_out/vocab.txt\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   loading file models/output/model_out/special_tokens_map.json\n",
      "05/06/2020 03:02:23 - INFO - transformers.tokenization_utils -   loading file models/output/model_out/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(str(FINETUNED_PATH), use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/06/2020 03:02:23 - INFO - root -   Loading features from cached file data/cache/cached_bert_train_multi_label_512_train.csv\n",
      "05/06/2020 03:02:34 - INFO - root -   Loading features from cached file data/cache/cached_bert_dev_multi_label_512_val.csv\n",
      "05/06/2020 03:02:37 - INFO - root -   Loading features from cached file data/cache/cached_bert_test_multi_label_512_test\n"
     ]
    }
   ],
   "source": [
    "databunch = BertDataBunch(args['data_dir'], LABEL_PATH, tokenizer=tokenizer, train_file='train.csv', val_file='val.csv',\n",
    "                          test_data='test.csv', label_file='labels.csv',\n",
    "                          text_col=\"comment_text\", label_col=label_cols,\n",
    "                          batch_size_per_gpu=args['train_batch_size'], max_seq_length=args['max_seq_length'], \n",
    "                          multi_gpu=args.multi_gpu, multi_label=True, model_type=args.model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/06/2020 03:02:38 - INFO - transformers.configuration_utils -   loading configuration file models/output/model_out/config.json\n",
      "05/06/2020 03:02:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 6,\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "05/06/2020 03:02:38 - INFO - transformers.modeling_utils -   loading weights file models/output/model_out/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "learner = BertLearner.from_pretrained_model(\n",
    "            databunch,\n",
    "            FINETUNED_PATH,\n",
    "            metrics=[],\n",
    "            device=device,\n",
    "            logger=None,\n",
    "            output_dir=None,\n",
    "            warmup_steps=0,\n",
    "            multi_gpu=False,\n",
    "            is_fp16=False,\n",
    "            multi_label=True,\n",
    "            logging_steps=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/06/2020 03:12:18 - INFO - root -   Writing example 0 of 63978\n",
      "05/06/2020 03:12:21 - INFO - root -   Writing example 10000 of 63978\n",
      "05/06/2020 03:12:23 - INFO - root -   Writing example 20000 of 63978\n",
      "05/06/2020 03:12:26 - INFO - root -   Writing example 30000 of 63978\n",
      "05/06/2020 03:12:28 - INFO - root -   Writing example 40000 of 63978\n",
      "05/06/2020 03:12:31 - INFO - root -   Writing example 50000 of 63978\n",
      "05/06/2020 03:12:34 - INFO - root -   Writing example 60000 of 63978\n"
     ]
    }
   ],
   "source": [
    "output = learner.predict_batch(list(pd.read_csv(DATA_PATH/\"test.csv\")['comment_text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output).to_csv(DATA_PATH/'output_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATA_PATH/'output_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame([{item[0]: item[1] for item in pred} for pred in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>threat</th>\n",
       "      <th>severe_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>0.973945</td>\n",
       "      <td>0.706804</td>\n",
       "      <td>0.650240</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.006624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>0.786447</td>\n",
       "      <td>0.066781</td>\n",
       "      <td>0.057240</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.004203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>0.989117</td>\n",
       "      <td>0.903307</td>\n",
       "      <td>0.732539</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.025135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.993964</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.100472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         toxic    insult   obscene  identity_hate    threat  severe_toxic\n",
       "1595  0.973945  0.706804  0.650240  0.012982       0.001271  0.006624    \n",
       "1596  0.000603  0.000252  0.000233  0.000228       0.000162  0.000158    \n",
       "1597  0.786447  0.066781  0.057240  0.050285       0.010325  0.004203    \n",
       "1598  0.989117  0.903307  0.732539  0.032157       0.003122  0.025135    \n",
       "1599  0.001438  0.000286  0.000374  0.000278       0.000155  0.000138    \n",
       "1600  0.993964  0.944056  0.980737  0.029374       0.005204  0.100472    "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.iloc[1595:1601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_PATH/\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>3915</td>\n",
       "      <td>06b32f1e5707afc9</td>\n",
       "      <td>U wot? U wot? U wot? U wot? U wot? U wot? U wot? U wot? I reverted it myself. No need to be such a douchebag.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>3920</td>\n",
       "      <td>06b52e91bc528426</td>\n",
       "      <td>\" \\n : The site does not meet WP:RS. Please also see our conflict of interest policy and our page regarding single-purpose accounts. If you continue to add the links, your account will be blocked.   \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>3921</td>\n",
       "      <td>06b59f9d648b3239</td>\n",
       "      <td>i know she was doing some porn lesbian stuff and i think everybody knows, but i consider it shouldn't appear on her wiki page cause it is not approppriate !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>3925</td>\n",
       "      <td>06b6f55efea0654f</td>\n",
       "      <td>==Somewhere== \\n Why are you being retarded? Your backstory is under fire. You are a b-h, so go fk it all. Fk You!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>3926</td>\n",
       "      <td>06b7562f03712738</td>\n",
       "      <td>::Well look, you don't have to preach to me about any of this... to be quite honest I'm actually on the fence with many of these changes, and there some I just flat out oppose.  But that doesn't mean that this arguement shouldn't get a fair, impartial reckoning.  It is clear that there are many predatory, greedy trial lawyers in the world.  It's  equally clear that there are many predatory, greedy corporations in the world.  And caught in the middle are industrial workers, white collar workers, doctors, patients and about a million lobbies on either side that deal their cards from the sleeve.  \\n\\n ::And the debate is not worthless, because we do live in a democracy.  Our laws aren't static, and are constantly up for reevaluation.  As you know, many of these kinds of changes have already been passed at the state level, which means that there are large groups of people who agree with their content.  My brother is both a Democrat and an Emergency MD, and recently had cause to rethink some of his positions on this when he saw his hospital being destaffed of specialists due to several colleagues being unable to pay their premiums.  It is a complex, multifaceted debate, and there are legitimate players on each side of it who want the same thing... better laws and better justice.  My $0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3927</td>\n",
       "      <td>06b75897e90137aa</td>\n",
       "      <td>Wow, you're a funny motherfucker, you know that? Don't waste your talent here, you should be on the goddamn radio. Seriously though, vandalizing wikipedia for kicks? You need a hobby. Read a book, go outside, jerk off, just do something productive.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                id  \\\n",
       "1595  3915        06b32f1e5707afc9   \n",
       "1596  3920        06b52e91bc528426   \n",
       "1597  3921        06b59f9d648b3239   \n",
       "1598  3925        06b6f55efea0654f   \n",
       "1599  3926        06b7562f03712738   \n",
       "1600  3927        06b75897e90137aa   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comment_text  \\\n",
       "1595  U wot? U wot? U wot? U wot? U wot? U wot? U wot? U wot? I reverted it myself. No need to be such a douchebag.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1596  \" \\n : The site does not meet WP:RS. Please also see our conflict of interest policy and our page regarding single-purpose accounts. If you continue to add the links, your account will be blocked.   \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1597  i know she was doing some porn lesbian stuff and i think everybody knows, but i consider it shouldn't appear on her wiki page cause it is not approppriate !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1598  ==Somewhere== \\n Why are you being retarded? Your backstory is under fire. You are a b-h, so go fk it all. Fk You!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "1599  ::Well look, you don't have to preach to me about any of this... to be quite honest I'm actually on the fence with many of these changes, and there some I just flat out oppose.  But that doesn't mean that this arguement shouldn't get a fair, impartial reckoning.  It is clear that there are many predatory, greedy trial lawyers in the world.  It's  equally clear that there are many predatory, greedy corporations in the world.  And caught in the middle are industrial workers, white collar workers, doctors, patients and about a million lobbies on either side that deal their cards from the sleeve.  \\n\\n ::And the debate is not worthless, because we do live in a democracy.  Our laws aren't static, and are constantly up for reevaluation.  As you know, many of these kinds of changes have already been passed at the state level, which means that there are large groups of people who agree with their content.  My brother is both a Democrat and an Emergency MD, and recently had cause to rethink some of his positions on this when he saw his hospital being destaffed of specialists due to several colleagues being unable to pay their premiums.  It is a complex, multifaceted debate, and there are legitimate players on each side of it who want the same thing... better laws and better justice.  My $0.02   \n",
       "1600  Wow, you're a funny motherfucker, you know that? Don't waste your talent here, you should be on the goddamn radio. Seriously though, vandalizing wikipedia for kicks? You need a hobby. Read a book, go outside, jerk off, just do something productive.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "      toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "1595  1      0             1        0       1       0              \n",
       "1596  0      0             0        0       0       0              \n",
       "1597  0      0             0        0       0       0              \n",
       "1598  1      0             1        0       1       0              \n",
       "1599  0      0             0        0       0       0              \n",
       "1600  1      0             1        0       1       0              "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[1595:1601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
